#!/usr/bin/env python
# Copyright (C) 2013 Ion Torrent Systems, Inc. All Rights Reserved
# pylint: disable=missing-docstring, line-too-long, bad-whitespace, trailing-whitespace

import sys
import os
import re
import subprocess
import json
import shutil
import time
import traceback
import tempfile
import copy
from optparse import OptionParser
from django.conf import settings
from django.template.loader import render_to_string
from ion.utils import compress # provided by ion-pipeline

# critical environment variables:
DIRNAME                     = '' # home directory for the plugin files
TSP_URLPATH_PLUGIN_DIR      = ''
TSP_FILEPATH_PLUGIN_DIR     = ''
STARTPLUGIN_JSON            = {}
ANALYSIS_DIR                = ''
OUTPUT_FILES                = []

# File names generated by the plugin
BASENAME_VARIANTS_XLS       = 'variants.xls'
BASENAME_ALLELES_XLS        = 'alleles.xls'
BASENAME_HOTSPOTS_XLS       = 'allele_counts.xls'
BASENAME_VARIANTS_VCF       = 'TSVC_variants.vcf'
BASENAME_GENOME_VCF         = 'TSVC_variants.genome.vcf'
BASENAME_PARAMETERS_JSON    = 'local_parameters.json'
HTML_BLOCK                  = 'variantCaller_block.html'    # Top report page block
HTML_RESULTS                = 'variantCaller.html'          # Main plugin page
BASENAME_VARIANT_COV_XLS    = 'variant_allele_counts.xls'


# DEVELOPMENT/DEBUG options:
# NOTE: the following should all be set to 0 in production mode
PLUGIN_DEV_KEEP_INTERMEDIATE_FILES = True   # use prior to PLUGIN_DEV_RECALL_VARIANTS=1 to re-process from temporary results
PLUGIN_DEV_SKIP_VARIANT_CALLING = False      # 1 to skip variant calling - use previous calls
SKIP_BAMFILE_VERSION_CHECK = False

# Minimum barcode BAM size required for variant calling. 50,000 bytes ~ 100-400 reads.
BCFILE_MIN_SIZE = 50000

def printtime(message, *args):
    if args:
        message = message % args
    print "[ " + time.strftime('%a %Y-%m-%d %X %Z') + " ] " + message
    sys.stdout.flush()
    sys.stderr.flush()


def unicode_cleanup(message):
    try:
        return str(message)
    except:
        return unicodedata.normalize('NFKD',unicode(message)).encode('ascii','ignore')

def run_command(command,description):
    printtime(' ')
    printtime('Task    : ' + description)
    printtime('Command : ' + command)
    printtime(' ')
    return subprocess.call(command,shell=True)

def execute_output(cmd):
    try:
        process = subprocess.Popen(cmd, stderr=subprocess.STDOUT, stdout=subprocess.PIPE, shell=True)
        return process.communicate()[0]
    except:
        traceback.print_exc()
        return ''


def generate_incomplete_report_page(output_html_filename, message, vc_options, autorefresh=False):
    run_name = ''
    if 'run_name' in vc_options: 
        run_name = vc_options['run_name']
    render_context = { 'run_name' : run_name, 'message' : message, 'autorefresh' : autorefresh,
                       'startplugin_json' : STARTPLUGIN_JSON}

    out = open(output_html_filename,'w')
    out.write(render_to_string('report_incomplete.html', render_context))
    out.close()


def generate_nonbarcoded_block_page(output_html_filename, message):
    out = open(output_html_filename,'w')
    css_str = """<link rel="stylesheet" media="all" href="/site_media/resources/bootstrap/css/bootstrap.min.css">"""
    out.write("<!DOCTYPE html><html>" + css_str + "<body><p>" + message + "</p></body></html>\n")
    out.close()


def generate_barcode_links_block(block_html_path, barcode_data, vc_options):

    render_context = {
        'barcode_data'          : barcode_data,
        'options'               : vc_options,
        'startplugin_json'      : STARTPLUGIN_JSON
    }

    out = open(block_html_path,'w')
    out.write(render_to_string('block_barcodes.html', render_context))
    out.close()



def generate_barcode_links_page (results_html_path, barcode_data, vc_options):

    render_context = {
        'barcode_data'     : barcode_data,
        'options'          : vc_options,
        'autorefresh'      : False,
        'startplugin_json' : STARTPLUGIN_JSON
    }

    for barcode in barcode_data:
        if barcode['status'] == 'in_progress':
            render_context['autorefresh'] = True

    out = open(results_html_path,'w')
    out.write(render_to_string('report_barcodes.html', render_context))
    out.close()



def add_output_file(file_type, filename, barcode=None, sample=None):
    file_info = {
        'type' : file_type,
        'filename' : os.path.basename(filename),
        'server_path' : os.path.join(TSP_FILEPATH_PLUGIN_DIR,filename),
        'download_path' : os.path.join(TSP_URLPATH_PLUGIN_DIR,filename)
    }
    if barcode is not None:
        file_info['barcode'] = barcode
    if sample is not None:
        file_info['sample'] = sample

    OUTPUT_FILES.append(file_info)

def package_vcf(vcf_file):
    run_command('bgzip -c ' + vcf_file + ' > ' + vcf_file + '.gz', 'Generate compressed vcf')
    run_command('tabix -p vcf ' + vcf_file + '.gz', 'Generate index for compressed vcf')

def prepare_hotspots(options):
    # create 3 files in TSP_FILEPATH_PLUGIN_DIR and re-evaluate 'has_hotspots' and 'hotspots_vcf'
    if options['has_hotspots']:
        if not os.path.exists(options['hotspots_bed_unmerged']):
            printtime('ERROR: Cannot locate hotspots file: ' +  options['hotspots_bed_unmerged'])
            return 1
        if not os.path.exists(options['hotspots_bed_merged']):
            printtime('ERROR: Cannot locate merged hotspots file: ' + options['hotspots_bed_merged'])
            return 1

        options['hotspots_bed_unmerged_local']     = os.path.join(TSP_FILEPATH_PLUGIN_DIR,os.path.basename(options['hotspots_bed_unmerged']))
        options['hotspots_bed_unmerged_leftalign'] = os.path.join(TSP_FILEPATH_PLUGIN_DIR,os.path.basename(options['hotspots_bed_unmerged'][:-4] + '.left.bed'))
        options['hotspots_vcf']                    = os.path.join(TSP_FILEPATH_PLUGIN_DIR,os.path.basename(options['hotspots_bed_unmerged'][:-4] + '.hotspot.vcf'))

        if not os.path.exists(options['hotspots_bed_unmerged_local']):
            shutil.copy(options['hotspots_bed_unmerged'], options['hotspots_bed_unmerged_local'])

            prepare_hotspots_command  = 'tvcutils prepare_hotspots'
            prepare_hotspots_command += '  --input-bed "%s"' % options['hotspots_bed_unmerged']
            prepare_hotspots_command += '  --reference "%s"' % options['reference_genome_fasta']
            prepare_hotspots_command += '  --left-alignment on'
            prepare_hotspots_command += '  --allow-block-substitutions on'
            prepare_hotspots_command += '  --output-bed "%s"' % options['hotspots_bed_unmerged_leftalign']
            prepare_hotspots_command += '  --output-vcf "%s"' % options['hotspots_vcf']
            if options['has_targets']:
                prepare_hotspots_command += '  --unmerged-bed "%s"' % options['targets_bed_unmerged']
            run_command(prepare_hotspots_command, 'Generate filtered, left-aligned, and merged hotspot VCF file')

            hotspot_file_empty = True
            try:
                fin = open(options['hotspots_vcf'], 'r')
                for line in fin:
                    if not line or line.startswith('#'):
                        continue
                    hotspot_file_empty = False
                    break
            except:
                traceback.print_exc()

            if hotspot_file_empty:
                printtime('Filtered hotspot file has no hotspot entries. Disabling hotspots')
                options['has_hotspots'] = False
                options['hotspots_vcf'] = ""
            else:
                #run_command('bgzip -c %s/hotspot.vcf > %s/hotspot.vcf.gz' % (TSP_FILEPATH_PLUGIN_DIR,TSP_FILEPATH_PLUGIN_DIR), 'Generate compressed hotspot vcf')
                #run_command('tabix -p vcf %s/hotspot.vcf.gz' % (TSP_FILEPATH_PLUGIN_DIR), 'Generate index for compressed hotspot vcf')
                add_output_file('hotspots_bed', os.path.basename(options['hotspots_bed_unmerged']))

def parse_to_dict(filein,sep=None):
    ret = {}
    if os.path.exists(filein):
        with open(filein) as fin:
            for line in fin:
                line = line.strip()
                # ignore lines being with non-alphanum (for comments, etc)
                if line == "" or not line[0].isalnum():
                    continue
                kvp = line.split(sep,1)
                if len(kvp) > 1:
                    ret[kvp[0].strip()] = kvp[1].strip()
    else:
        printtime("parse_to_dict() could not open "+filein)
    return ret

def get_options(json_in):
    ''' Attempt to get plugin options '''

    options = {}
    try:
        options['parameters']       = copy.deepcopy(json_in['pluginconfig'])
        configuration               = options['parameters']['meta']['configuration']
    except:
        if 'barcodes' in json_in['pluginconfig']:
            json_in['pluginconfig']['meta'] = copy.deepcopy(json_in['pluginconfig']['barcodes'][0]['json_in']['pluginconfig']['meta'])
        try:
            options['parameters']       = copy.deepcopy(json_in['pluginconfig'])
            configuration               = options['parameters']['meta']['configuration']
        except:
            return {'error':'Automatic analysis was not performed. Plugin does not appear to be configured.'}

    options['has_error_motifs'] = True
    options['error_motifs'] = os.path.join(DIRNAME,'share/TVC/sse/motifset.txt')
    try:
        expmeta = json_in['expmeta']
        plan = json_in['plan']
        if plan['samplePrepKitName'] == 'Ion AmpliSeq Exome Kit' and expmeta['chiptype'] == 'P1.1.17' and plan['sequencekitname'] == 'IonProtonIHiQ':
            options['error_motifs'] =  os.path.join(DIRNAME,'share/TVC/sse/ampliseqexome_germline_p1_hiq_motifset.txt')
        if plan['samplePrepKitName'] == 'Ion AmpliSeq Exome Kit' and plan['sequencekitname'] == 'Ion S5 Sequencing Kit':
            options['error_motifs'] =  os.path.join(DIRNAME,'share/TVC/sse/ampliseqexome_germline_p1_hiq_motifset.txt')
        if plan['samplePrepKitName'] == 'Ion AmpliSeq Exome Kit' and expmeta['chiptype'] == '540' and plan['sequencekitname'] == 'IonProtonIHiQ':
            options['error_motifs'] =  os.path.join(DIRNAME,'share/TVC/sse/ampliseqexome_germline_p1_hiq_motifset.txt')
    except:
        pass

    with open(DIRNAME+'/pluginMedia/parameter_sets/parameter_sets.json','r') as fin:
        built_in_parameters = json.load(fin, parse_float=str)

    for reload_parameters in built_in_parameters:
        if configuration not in reload_parameters["meta"]["replaces"]:
            continue

        options['original_parameters'] = copy.deepcopy(json_in['pluginconfig'])
        options['parameters'] = reload_parameters

        if 'meta' not in options['parameters']:
            options['parameters']['meta'] = {}
        if 'configuration' not in options['parameters']['meta']:
            options['parameters']['meta']['configuration'] = configuration

        options["original_config_line1"] = options['original_parameters']['meta'].get('name','Legacy '+configuration)
        options["original_config_line2"] = ''
        if options['original_parameters']['meta'].get('configuration',''):
            options["original_config_line2"] += options['original_parameters']['meta']['configuration'] + ', '
        options["original_config_line2"] += 'TS version: ' + options['original_parameters']['meta'].get('ts_version','5.0')
        break

    options["config_line1"] = options['parameters']['meta'].get('name','Legacy '+configuration)
    options["config_line2"] = ''
    if options['parameters']['meta'].get('configuration',''):
        options["config_line2"] += options['parameters']['meta']['configuration'] + ', '
    options["config_line2"] += 'TS version: ' + options['parameters']['meta'].get('ts_version','5.0')

    # Ensure nonstandard unicode characters are eliminated from config_line1, and original_config_line1

    options["config_line1"] = unicode_cleanup(options["config_line1"])
    if 'original_config_line1' in options:
        options["original_config_line1"] = unicode_cleanup(options["original_config_line1"])



    options['parameters']['meta']['tvcargs'] = json_in['pluginconfig']['meta'].get('tvcargs','')
    if not options['parameters']['meta']['tvcargs']:
        options['parameters']['meta']['tvcargs'] = 'tvc'
    # Call tvc -v to get the version string
    tvc_args = options['parameters'].get('meta',{}).get('tvcargs','tvc')
    if tvc_args == 'tvc' and os.path.exists(DIRNAME + '/tvc'):   # try local binary first, then go to global one
        tvc_args = DIRNAME + '/tvc'
    options['tvc_version'] = execute_output(tvc_args + ' -v').splitlines()[0]
    if options['tvc_version'].endswith('- Torrent Variant Caller'):
        options['tvc_version'] = options['tvc_version'][:-24].strip()
    if tvc_args == "tmol":
        options['tvc_version'] = "Tagged Molecule Caller v.0.3.3"

    # These two values are needed to display the 'Output Directory' in the HTML pages
    options['plugin_name']               = json_in['runinfo'].get('plugin_name','')
    options['pluginresult']              = json_in['runinfo'].get('pluginresult','')

    options['run_name']                  = json_in['expmeta'].get('run_name','Current run')
    options['has_barcodes']              = json_in['expmeta'].get('barcodeId','') != ''

    options['trim_reads']   = json_in['pluginconfig']['meta'].get('trimreads',True)
    options['multisample'] = json_in['pluginconfig'].get('multisample',False)

    try:
        options['start_mode']               = 'Manual start'
        options['library_type']             = json_in['pluginconfig']['meta']['librarytype']
        options['reference_genome_name']    = json_in['pluginconfig']['meta']['reference']
        options['targets_bed_unmerged']     = json_in['pluginconfig']['meta']['targetregions']
        options['hotspots_bed_unmerged']    = json_in['pluginconfig']['meta']['targetloci']
    except:
        options['start_mode']               = 'Auto start'
        options['library_type']             = json_in.get('plan',{}).get('runType',None)
        options['reference_genome_name']    = json_in['runinfo'].get('library','')
        options['targets_bed_unmerged']     = json_in.get('plan',{}).get('bedfile','')
        options['hotspots_bed_unmerged']    = json_in.get('plan',{}).get('regionfile','')
        
    if options['reference_genome_name'] == "":
        options['start_mode']               = 'Auto start'
        options['library_type']             = json_in.get('plan',{}).get('runType',None)
        options['reference_genome_name']    = json_in['runinfo'].get('library','')
        options['targets_bed_unmerged']     = json_in.get('plan',{}).get('bedfile','')
        options['hotspots_bed_unmerged']    = json_in.get('plan',{}).get('regionfile','')

    cleanup_options(options)

    return options


def cleanup_options(options):

    options['reference_genome_fasta']       = '/results/referenceLibrary/tmap-f3/' + options['reference_genome_name'] + '/' + options['reference_genome_name'] + '.fasta' #TODO

    if not options['targets_bed_unmerged'] or options['targets_bed_unmerged'] == "none":
        options['targets_bed_unmerged']     = ""
        options['targets_bed_merged']       = ""
        options['targets_name']             = ""
        options['trim_reads']               = False
    else:
        options['targets_bed_merged']       = options['targets_bed_unmerged'].replace('/unmerged/detail/','/merged/plain/')
        options['targets_name']             = os.path.basename(options['targets_bed_unmerged'])[:-4]

    if not options['hotspots_bed_unmerged'] or options['hotspots_bed_unmerged'] == "none":
        options['hotspots_bed_unmerged']    = ""
        options['hotspots_bed_merged']      = ""
        options['hotspots_name']            = ""
    else:
        options['hotspots_bed_merged']      = options['hotspots_bed_unmerged'].replace('/unmerged/detail/','/merged/plain/')
        options['hotspots_name']            = os.path.basename(options['hotspots_bed_unmerged'])[:-4]

    options['has_targets']                  = options['targets_name']
    options['has_hotspots']                 = options['hotspots_name']

    reference_genome_fasta_local = os.path.join(TSP_FILEPATH_PLUGIN_DIR, os.path.basename(options['reference_genome_fasta']))
    if not os.path.lexists(reference_genome_fasta_local):
        os.symlink(options['reference_genome_fasta'], reference_genome_fasta_local)

    reference_genome_fai_local = os.path.join(TSP_FILEPATH_PLUGIN_DIR, os.path.basename(options['reference_genome_fasta']) + '.fai')
    if not os.path.lexists(reference_genome_fai_local):
        os.symlink(options['reference_genome_fasta'] + '.fai', reference_genome_fai_local)

    # Get local copy of BED files (may be deleted from system later)
    if options['has_targets']:
        if not os.path.exists( options['targets_bed_unmerged']):
            printtime('ERROR: Cannot locate target regions file: ' +  options['targets_bed_unmerged'])
            return 1
        if not os.path.exists(options['targets_bed_merged']):
            printtime('ERROR: Cannot locate merged target regions file: ' + options['targets_bed_merged'])
            return 1
        target_file = "%s/%s" % (TSP_FILEPATH_PLUGIN_DIR,os.path.basename(options['targets_bed_unmerged']))
        if not os.path.exists(target_file):
            shutil.copy(options['targets_bed_unmerged'], target_file)

            add_output_file('target_regions_bed', os.path.basename(options['targets_bed_unmerged']))

    if options['library_type'] in ["wholegenome",'WGNM','GENS']:
        options['library_type'] = "Whole Genome"
        options['trim_reads']   = False
    elif options['library_type'] in ["ampliseq",'AMPS','AMPS_EXOME','AMPS_DNA_RNA','AMPS_DNA']:
        options['library_type'] = "AmpliSeq"
    elif options['library_type'] in ["targetseq",'TARS']:
        options['library_type'] = "TargetSeq"
        options['trim_reads']   = False
    elif options['library_type'] in ["tagseq", 'tag_sequencing', 'TAG_SEQUENCING']:
        options['library_type'] = "tagseq"
        options['trim_reads']   = False
    elif not options['library_type']:
        return {'error':'Automatic analysis was not performed. Cannot determine library type from plan.'}
    else:
        return {'error':'Automatic analysis was not performed. Library type "%s" is not supported.' % options['library_type']}

    if options['library_type'] == "AmpliSeq" and not options['has_targets']:
        return {'error':'Analysis aborted: AmpliSeq runs must have target regions specified.'}

    if options['library_type'] == "tagseq" and not options['has_targets']:
        return {'error':'Analysis aborted: TagSequencing runs must have target regions specified.'}
    if options['library_type'] == "tagseq" and not options['has_hotspots']:
        return {'error':'Analysis aborted: TagSequencing runs must have hotspot file specified.'}

    # These keys are not part of the parameter set
    if 'librarytype' in options['parameters']['meta']:
        del options['parameters']['meta']['librarytype']
    if 'targetregions_id' in options['parameters']['meta']:
        del options['parameters']['meta']['targetregions_id']
    if 'targetregions' in options['parameters']['meta']:
        del options['parameters']['meta']['targetregions']
    if 'targetregions_merge' in options['parameters']['meta']:
        del options['parameters']['meta']['targetregions_merge']
    if 'targetloci_id' in options['parameters']['meta']:
        del options['parameters']['meta']['targetloci_id']
    if 'targetloci' in options['parameters']['meta']:
        del options['parameters']['meta']['targetloci']
    if 'targetloci_merge' in options['parameters']['meta']:
        del options['parameters']['meta']['targetloci_merge']
    if 'user_selections' in options['parameters']['meta']:
        del options['parameters']['meta']['user_selections']

def get_bam_reference_short_name(bam):
    proc = subprocess.Popen(['samtools', 'view', '-H', bam], stdout=subprocess.PIPE)
    lines = proc.stdout.readlines()
    for line in lines:
        if line.startswith('@PG\tID:tmap'):
            value = line.split(' ')
            fasta = [i for i in value if i.endswith('.fasta')]
            assert len(fasta) == 1, '>1 fasta\n'
            fasta = fasta[0]
            break
    short_name = re.search(".*/(.*)\.fasta", fasta).group(1)
    return short_name
    
def tmap(cmd, new_aligned_bam):
    cmd += " | samtools sort -m 1000M -l1 -@12 - " + new_aligned_bam[:-4]
    printtime(cmd)
    subprocess.call(cmd,shell=True)
    cmd = "samtools index " + new_aligned_bam
    printtime(cmd)
    subprocess.call(cmd,shell=True)
    plan_file = TSP_FILEPATH_PLUGIN_DIR + "/../../ion_params_00.json"
    mark_duplicates = False
    with open(plan_file,'r') as fin:
        plan_data = json.load(fin, parse_float=str)
        mark_duplicates = plan_data.get('experimentAnalysisSettings',{}).get('isDuplicateReads',False)
    printtime("mark_duplicates = " + str(mark_duplicates))
    if mark_duplicates:
        cmd = "cp " + new_aligned_bam + " " + new_aligned_bam + ".temp.bam"
        printtime(cmd)
        subprocess.call(cmd,shell=True)
        cmd = "samtools index " + new_aligned_bam + ".temp.bam"
        printtime(cmd)
        subprocess.call(cmd,shell=True)
        cmd = "BamDuplicates -i " + new_aligned_bam + ".temp.bam -o " + new_aligned_bam
        printtime(cmd)
        subprocess.call(cmd,shell=True)
        cmd = "samtools index " + new_aligned_bam
        printtime(cmd)
        subprocess.call(cmd,shell=True)
        cmd = "rm " + new_aligned_bam + ".temp.bam"
        printtime(cmd)
        subprocess.call(cmd,shell=True)
        cmd = "rm " + new_aligned_bam + ".temp.bam.bai"
        printtime(cmd)
        subprocess.call(cmd,shell=True)
    return new_aligned_bam

def run_tmap(aligned_bam, new_reference, unaligned_bam, parameters, new_aligned_bam):
    files_string = " -n 24 -f " + new_reference + " -r " + unaligned_bam + " -v -Y -u --prefix-exclude 5 -o 2 "
    cmd = parameters.replace(" ... ", files_string)
    out = ""
    try:
        proc = subprocess.Popen(["samtools", "view", "-H", aligned_bam],stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        (out, err) = proc.communicate()
    except subprocess.CalledProcessError:
        printtime(dir + " variant_caller_pipeline run failed.")
    except OSError:
        printtime(dir + " variant_caller_pipeline run failed.")
    pos = out.find("ID:tmap")
    if pos == -1:
        return aligned_bam
    else:
        out = out[pos:]
        pos = out.find ("CL:")
        if pos == -1:
            return aligned_bam
        out = out[pos + 3:]
        pos = out.find("	VN:")
        if pos == -1:
            return aligned_bam
        out = out[:pos]
        pos = out.find("\n")
        if pos != -1:
            out = out[:pos]
        out = out.replace(" -i ", " -r ")
        
        q_param = ""
        posq = out.find(" -q ")
        if posq != -1:
            posq += 4
            q_param = " -q "
            while out[posq] == " ":
                posq += 1
            while out[posq] != " ":
                q_param += out[posq]
                posq += 1
        files_string = q_param + files_string
        cmd = parameters.replace(" ... ", files_string)

        posq = out.find(" -q ")
        pos1 = out.find(" -n ")
        pos2 = out.find(" -f ")
        pos3 = out.find(" -r ")
        pos4 = out.find(" -o ")
        if pos1 == -1 or pos2 == -1 or pos3 == -1 or pos4 == -1:
            return tmap(cmd, new_aligned_bam)
        if pos1 > pos2 or pos2 > pos3 or pos3 > pos4:
            return tmap(cmd, new_aligned_bam)
        pos = out.find(" -n ")
        if posq != -1 and posq < pos:
            pos = posq
        part_1 = out[:pos]
        pos = out.find(" -f")
        reference = out[pos+4:]
        pos = reference.find(" -r")
        reference = reference[:pos]
        pos = out.find(" -o 2 ")
        part_2 = out[pos+6:]
        oldcmd = "tmap " + part_1 + files_string + part_2
        cmd = "tmap " + part_1 + files_string + part_2
        if reference != new_reference or parameters != "":
            if parameters.find(" ... ") != -1: 
                cmd = parameters.replace(" ... ", files_string)
            elif parameters != "": 
                part_2 = parameters
                cmd = "tmap " + part_1 + files_string + part_2
            if reference != new_reference or cmd != oldcmd:
                return tmap(cmd, new_aligned_bam)
    return aligned_bam
    
def combine_files(combinedfilename, myfile):
    try:
        file_in = open(myfile, "r")
        if os.path.isfile(combinedfilename):
            file_out = open(combinedfilename, "a")
            for line in file_in:
                if not line.startswith("#"): 
                    file_out.write(line)
            file_out.close()
        else:
            file_out = open(combinedfilename, "w")
            for line in file_in:
                file_out.write(line)
            file_out.close()
        file_in.close()
    except:
        pass

def print_options(vc_options, parameters_file):
    printtime('Variant Caller plugin run options:')
    printtime('  Plugin name                : ' + STARTPLUGIN_JSON['runinfo'].get('plugin_name',''))
    printtime('  Plugin start mode          : ' + vc_options['start_mode'])
    printtime('  Variant Caller version     : ' + vc_options['tvc_version'])
    printtime('  Run is barcoded            : ' + str(vc_options['has_barcodes']))
    printtime('  Genome                     : ' + vc_options['reference_genome_name'])
    printtime('  Library Type               : ' + vc_options['library_type'])
    printtime('  Target Regions             : ' + (vc_options['targets_name'] if vc_options['has_targets'] else 'Not using'))
    printtime('  Hotspots                   : ' + (vc_options['hotspots_name'] if vc_options['has_hotspots'] else 'Not using'))
    if 'original_parameters' in vc_options:
        printtime('  Requested Parameters       : ' + vc_options["original_config_line1"])
        printtime('                               ' + vc_options["original_config_line2"])
        printtime('  Auto-Updated Parameters    : ' + vc_options["config_line1"])
        printtime('                               ' + vc_options["config_line2"])
    else:
        printtime('  Used Parameters            : ' + vc_options['config_line1'])
        printtime('                               ' + vc_options["config_line2"])

    printtime('  Trim Reads                 : ' + str(vc_options['trim_reads']))

    printtime('')
    printtime('Used files:')
    printtime('  Reference Genome           : ' + vc_options['reference_genome_fasta'])
    printtime('  Parameters file            : ' + os.path.join(TSP_FILEPATH_PLUGIN_DIR,parameters_file))
    if 'parameters_source' in vc_options:
        printtime('  Parameters source file     : ' + vc_options['parameters_source'])

    if vc_options['has_targets']:
        printtime('  Target unmerged BED        : ' + vc_options['targets_bed_unmerged'])
        printtime('  Target merged BED          : ' + vc_options['targets_bed_merged'])
    if vc_options['has_hotspots']:
        printtime('  Hotspots unmerged BED      : ' + vc_options['hotspots_bed_unmerged'])
        printtime('  Hotspots merged BED        : ' + vc_options['hotspots_bed_merged'])
    printtime('  Multi-sample               : ' + str(vc_options['multisample']))
    printtime('')
    
def get_configurations():
    barcoded_run = STARTPLUGIN_JSON['expmeta'].get('barcodeId','') != ''
    configured_run = 'barcodes' in STARTPLUGIN_JSON['pluginconfig']
    configurations = {}
    if configured_run:
        printtime("Run is using configurations")
        for element in STARTPLUGIN_JSON['pluginconfig']['barcodes']:
            configuration_name = element['json']['pluginconfig'].get('meta',{}).get('configuration_name','')
            configurations[configuration_name] = {}
            configurations[configuration_name]['json']  = copy.deepcopy(element['json'])
            configurations[configuration_name]['json'] ['plan'] = copy.deepcopy(STARTPLUGIN_JSON['plan'])
            configurations[configuration_name]['json'] ['expmeta'] = copy.deepcopy(STARTPLUGIN_JSON['expmeta'])
            configurations[configuration_name]['json'] ['runinfo'] = copy.deepcopy(STARTPLUGIN_JSON['runinfo'])
            configurations[configuration_name]['bams'] = []
    else:
        printtime("Run is not using configurations")
        configuration_name = STARTPLUGIN_JSON['pluginconfig'].get('meta',{}).get('configuration_name','')
        configurations[configuration_name] = {}
        configurations[configuration_name]['json'] = copy.deepcopy(STARTPLUGIN_JSON)
        configurations[configuration_name]['bams'] = []

    if barcoded_run:
        printtime('Run is using barcodes')
        if configured_run:
            barcodes = {}
            if os.path.exists(ANALYSIS_DIR + '/barcodeList.txt'):
                with open(ANALYSIS_DIR + '/barcodeList.txt','r') as bc_list_file:
                    for line in bc_list_file:
                        if not line.startswith('barcode '):
                            continue
                        name = line.split(',')[1]
                        barcodes[os.path.join(ANALYSIS_DIR, name + '_rawlib.bam')] = name
            for element in STARTPLUGIN_JSON['pluginconfig']['barcodes']:
                bam = {}
                try:
                    bam['name'] = barcodes[os.path.join(ANALYSIS_DIR, element['bam'])]
                except:
                    continue
                bam['file'] = os.path.join(ANALYSIS_DIR, element['bam'])
                bam['status'] = 'queued'
                configuration_name = element['json']['pluginconfig'].get('meta',{}).get('configuration_name','')
                if os.path.exists(bam['file']):
                    if os.stat(bam['file']).st_size >= BCFILE_MIN_SIZE:
                        configurations[configuration_name]['bams'].append(bam)
        else:
            if os.path.exists(ANALYSIS_DIR + '/barcodeList.txt'):
                with open(ANALYSIS_DIR + '/barcodeList.txt','r') as bc_list_file:
                    for line in bc_list_file:
                        if not line.startswith('barcode '):
                            continue
                        bam = {}
                        bam['name'] = line.split(',')[1]
                        bam['file'] = os.path.join(ANALYSIS_DIR, bam['name'] + '_rawlib.bam')
                        bam['status'] = 'queued'
                        configuration_name = STARTPLUGIN_JSON['pluginconfig'].get('meta',{}).get('configuration_name','')
                        if os.path.exists(bam['file']):
                            if os.stat(bam['file']).st_size >= BCFILE_MIN_SIZE:
                                configurations[configuration_name]['bams'].append(bam)
    else:
        printtime('Run is not using barcodes')
        bam = {}
        bam['name'] = 'default'
        bam['file'] = os.path.join(ANALYSIS_DIR, 'rawlib.bam')
        bam['status'] = 'queued'
        if configured_run:
            configuration_name = STARTPLUGIN_JSON['pluginconfig']['barcodes'][0]['json']['pluginconfig'].get('meta',{}).get('configuration_name','')
        else:
            configuration_name = STARTPLUGIN_JSON['pluginconfig'].get('meta',{}).get('configuration_name','') 
        if os.path.exists(bam['file']):
            if os.stat(bam['file']).st_size >= BCFILE_MIN_SIZE:  
                configurations[configuration_name]['bams'].append(bam)

    printtime('')
    for configuration_name in configurations:
        for bam in configurations[configuration_name]['bams']:
            printtime(configuration_name + "\t" + bam['name'])
    printtime('')

    multisample =  STARTPLUGIN_JSON['pluginconfig'].get('multisample',False)
    printtime('Multisample run: ' + str(multisample))
    printtime('')
    return (configured_run, barcoded_run, configurations, multisample)

def process_reference(configuration, bam):
    tmap_args = configuration['json']['pluginconfig'].get('meta',{}).get('tmapargs','')
    true_reference = get_bam_reference_short_name(bam['file'])
    if true_reference != configuration['options']['reference_genome_name'] or tmap_args != "":
        if true_reference != configuration['options']['reference_genome_name']:
            printtime('Detected barcode ' + bam['name'] + ' : Bam reference ' + true_reference + ' different from run reference ' + configuration['options']['reference_genome_name'])
        # TS-11423 Use previously aligned bam as unaligned bam. Previous alignment will be stripped off by tmap
        unaligned_bam = bam['file']
        new_aligned_bam = os.path.join(TSP_FILEPATH_PLUGIN_DIR, bam['name'] + '_rawlib.' + 'realigned' + '.bam')
        bam['file'] = run_tmap(bam['file'], configuration['options']['reference_genome_fasta'], unaligned_bam, tmap_args, new_aligned_bam)
                
def make_directories(barcoded_run, vc_pipeline_directory, configuration):
    if not os.path.exists(vc_pipeline_directory):
        os.makedirs(vc_pipeline_directory)
        
    for bam in configuration['bams']:
        if barcoded_run:
            results_directory = TSP_FILEPATH_PLUGIN_DIR + '/' + bam['name']
        else:
            results_directory = TSP_FILEPATH_PLUGIN_DIR
        if not os.path.exists(results_directory):
            os.makedirs(results_directory)
        bam['results_directory'] = results_directory

def get_untrimmed_bams(configuration, bams):
    untrimmed_bams = ''
    for bam in bams:
        basename_input_bam   = os.path.basename(bam['file'])
        untrimmed_bam = os.path.join(bam['results_directory'],basename_input_bam)

        if bam['file'] != untrimmed_bam:
            os.symlink(bam['file'], untrimmed_bam)
            os.symlink(bam['file']+'.bai', untrimmed_bam + '.bai')

        assert not (configuration['options']['trim_reads'] and not configuration['options']['has_targets']), "Read trimming enabled but targets BED not provided"
        if untrimmed_bams != '':
            untrimmed_bams += ","
        untrimmed_bams += untrimmed_bam
    return untrimmed_bams

def variant_caller_pipeline(barcoded_run, multisample, configuration_name, configuration, bams):
    if configuration_name == 'default' and not barcoded_run:
        vc_pipeline_directory = TSP_FILEPATH_PLUGIN_DIR
    else:
        vc_pipeline_directory = TSP_FILEPATH_PLUGIN_DIR + '/' + configuration_name
    for bam in bams:
        bam['vc_pipeline_directory'] = vc_pipeline_directory

    make_directories(barcoded_run, vc_pipeline_directory, configuration)
    if (multisample):
        multisample_processed_bam = vc_pipeline_directory + '/multisample_processed.bam'
    else:
        multisample_processed_bam = os.path.join(vc_pipeline_directory,os.path.basename(bams[0]['file'])[:-4] + '_processed.bam')
    untrimmed_bams = get_untrimmed_bams(configuration, bams)
        
    prepare_hotspots(configuration['options'])
    if configuration['options']['library_type'] == 'tagseq':
        sample_name = 'none'
        bam_name = os.path.basename(bams[0]['file'])
        if bam_name.endswith('.realigned.bam'):
            bam_name = bam_name[:-14] + '.bam'
        fin = open(os.path.join(TSP_FILEPATH_PLUGIN_DIR,'barcodes.json'))
        barcodes_json = json.load(fin)
        fin.close()
        for key in barcodes_json:
            if (barcodes_json[key]['bam_file'] == bam_name):
                sample_name = barcodes_json[key]['sample']
        variantcaller_command        = '%s/bin/tmol_pipeline.py' % DIRNAME
        variantcaller_command        += ' --sample-name "%s" ' % sample_name
        #variantcaller_command        = '%s/bin/variant_caller_pipeline.py' % DIRNAME
    else:
        variantcaller_command        = '%s/bin/variant_caller_pipeline.py' % DIRNAME
    variantcaller_command   +=     '  --input-bam "%s"' % untrimmed_bams
 
    if configuration['options']['library_type'] != 'tagseq' and configuration['options']['trim_reads']:
        variantcaller_command   += '  --primer-trim-bed "%s"' % configuration['options']['targets_bed_unmerged']
        variantcaller_command   += '  --postprocessed-bam "%s"' % multisample_processed_bam

    variantcaller_command       += '  --reference-fasta "%s"' % configuration['options']['reference_genome_fasta']
    variantcaller_command       += '  --output-dir "%s"' % vc_pipeline_directory
    variantcaller_command       += '  --parameters-file "%s"' % configuration['options']['parameters_file']
    variantcaller_command       += '  --bin-dir "%s/bin"' % DIRNAME
    if configuration['options']['has_targets']:
        variantcaller_command   += '  --region-bed "%s"' % configuration['options']['targets_bed_merged']
        if configuration['options']['library_type'] == 'tagseq':
            variantcaller_command   += '  --region-bed "%s"' % configuration['options']['targets_bed_unmerged']
    if configuration['options']['has_hotspots']:
        variantcaller_command   += '  --hotspot-vcf "%s"' % configuration['options']['hotspots_vcf']
    if configuration['options']['has_error_motifs']:
        variantcaller_command   += '  --error-motifs "%s"' % configuration['options']['error_motifs']
    if configuration['options']['library_type'] != 'tagseq':
        variantcaller_command       += '  --generate-gvcf on'

    if PLUGIN_DEV_SKIP_VARIANT_CALLING:
        printtime('Skipping calling variants on mapped reads...')
    else:
        run_command(variantcaller_command,'Execute variant caller script')
    if configuration['options']['library_type'] == 'tagseq':
        if configuration['options']['has_targets']:
            run_command("cp '" + configuration['options']['targets_bed_unmerged'] + "' " + os.path.join(vc_pipeline_directory,'effective_regions.bed'), "Copy effective_regions.bed")

def split_results(barcoded_run, configuration_name, configuration, bams):
    vc_pipeline_directory = TSP_FILEPATH_PLUGIN_DIR + '/' + configuration_name
    multisample_processed_bam = vc_pipeline_directory + '/multisample_processed.bam'
    if len(bams) == 1:
        run_command("cp " + os.path.join(vc_pipeline_directory,'small_variants.vcf') + " " + os.path.join(vc_pipeline_directory,'small_variants_1.vcf'), "Copy small_variants.vcf")
        run_command("cp " + os.path.join(vc_pipeline_directory,'small_variants_filtered.vcf') + " " + os.path.join(vc_pipeline_directory,'small_variants_filtered_1.vcf'), "Copy small_variants_filtered.vcf")
        run_command("cp " + os.path.join(vc_pipeline_directory,'TSVC_variants.vcf') + " " + os.path.join(vc_pipeline_directory,'TSVC_variants_1.vcf'), "Copy TSVC_variants.vcf")
        run_command("cp " + os.path.join(vc_pipeline_directory,'TSVC_variants.genome.vcf') + " " + os.path.join(vc_pipeline_directory,'TSVC_variants.genome_1.vcf'), "Copy TSVC_variants.genome.vcf")
    else:
        cmd = 'tvcutils split_vcf'
        cmd += ' --input-vcf ' + os.path.join(vc_pipeline_directory,'small_variants.vcf')
        cmd += ' --out-dir ' + vc_pipeline_directory
        run_command(cmd, 'Split small_variants.vcf')
        cmd = 'tvcutils split_vcf'
        cmd += ' --input-vcf ' + os.path.join(vc_pipeline_directory,'small_variants_filtered.vcf')
        cmd += ' --out-dir ' + vc_pipeline_directory
        run_command(cmd, 'Split small_variants_filtered.vcf')
        cmd = 'tvcutils split_vcf'
        cmd += ' --input-vcf ' + os.path.join(vc_pipeline_directory,'TSVC_variants.vcf')
        cmd += ' --out-dir ' + vc_pipeline_directory
        run_command(cmd, 'Split TSVC_variants.vcf')
        cmd = 'tvcutils split_vcf'
        cmd += ' --input-vcf ' + os.path.join(vc_pipeline_directory,'TSVC_variants.genome.vcf')
        cmd += ' --out-dir ' + vc_pipeline_directory
        run_command(cmd, 'Split TSVC_variants.genome.vcf')
    dataset_num = 1
    for bam in bams:
        if barcoded_run:
            basename_variants_vcf  = 'TSVC_variants_%s.vcf' % bam['name']
            basename_genome_vcf    = 'TSVC_variants_%s.genome.vcf' % bam['name']
            results_directory      = TSP_FILEPATH_PLUGIN_DIR + '/' + bam['name']
        else:
            basename_variants_vcf  = BASENAME_VARIANTS_VCF
            basename_genome_vcf    = BASENAME_GENOME_VCF
            results_directory      = TSP_FILEPATH_PLUGIN_DIR

        if not os.path.exists(results_directory):
            os.makedirs(results_directory)
        if configuration['options']['library_type'] == 'tagseq':
            if not os.path.lexists(os.path.join(results_directory,"tmol.stats.txt")):
                os.symlink(os.path.join(vc_pipeline_directory,'tmol.stats.txt'), os.path.join(results_directory,"tmol.stats.txt"))
        if os.path.exists(os.path.join(vc_pipeline_directory,"effective_regions.bed")):
            if not os.path.lexists(os.path.join(results_directory,"effective_regions.bed")):
                os.symlink(os.path.join(vc_pipeline_directory,'effective_regions.bed'), os.path.join(results_directory,"effective_regions.bed"))
        if not os.path.lexists(os.path.join(results_directory,"small_variants.vcf")):
            os.symlink(os.path.join(vc_pipeline_directory,'small_variants_' + str(dataset_num) + '.vcf'), os.path.join(results_directory,"small_variants.vcf"))
        if not os.path.lexists(os.path.join(results_directory,"small_variants_filtered.vcf")):
            os.symlink(os.path.join(vc_pipeline_directory,'small_variants_filtered_' + str(dataset_num) + '.vcf'), os.path.join(results_directory,"small_variants_filtered.vcf"))
        if not os.path.lexists(os.path.join(results_directory,BASENAME_VARIANTS_VCF)):
            os.symlink(os.path.join(vc_pipeline_directory,'TSVC_variants_' + str(dataset_num) + '.vcf'), os.path.join(results_directory,BASENAME_VARIANTS_VCF))

        basename_input_bam   = os.path.basename(bam['file'])
        processed_bam = os.path.join(results_directory,basename_input_bam[:-4] + '_processed.bam')
 
        if configuration['options']['trim_reads']:
            read_group_id = ''
            proc = subprocess.Popen(['samtools', 'view', '-H', bam['file']], stdout=subprocess.PIPE)
            lines = proc.stdout.readlines()
            filename = results_directory + '/read_id.txt'
            fout = open(filename, 'w')
            for line in lines:
                if line.startswith('@RG\tID:'):
                    read_group_id = line[7:].split('\t')[0]
                    fout.write(read_group_id + '\n')
            fout.close()
            run_command("samtools view -bhR " + filename + " " + multisample_processed_bam + " > " + processed_bam, "Split multisample processed bam")
            run_command("samtools index " + processed_bam, "Index processed bam")
        else:
            processed_bam = bam['file']
        
        cmd = "samtools depth "
        if configuration['options']['has_targets']:
            cmd += "-b '" + configuration['options']['targets_bed_unmerged'] + "' "
        cmd += processed_bam + " | "
        cmd += "tvcutils unify_vcf "
        cmd += "--novel-tvc-vcf " + os.path.join(vc_pipeline_directory,'TSVC_variants_' + str(dataset_num) + '.vcf') + " "
        cmd += "--output-vcf " + os.path.join(results_directory,basename_variants_vcf) + ".gz "
        cmd += "--reference-fasta '" + configuration['options']['reference_genome_fasta'] + "' "
        if configuration['options']['has_targets']:
            cmd += "--target-file '" + configuration['options']['targets_bed_unmerged'] + "' "
        if configuration['options']['has_hotspots']:
            cmd += "--hotspot-annotation-vcf '" + configuration['options']['hotspots_vcf'] + "' "
        cmd += "--input-depth stdin "
        if configuration['options']['parameters'] and 'gen_min_coverage' in configuration['options']['parameters'].get('freebayes', {}):
            cmd +='--min-depth ' + str(configuration['options']['parameters']['freebayes']['gen_min_coverage']) 
        run_command(cmd, "Create genome vcf")
        run_command("gzip -dcf " + os.path.join(results_directory,basename_variants_vcf) + ".gz > " + os.path.join(results_directory,basename_variants_vcf), "unzip vcf")
        run_command("gzip -dcf " + os.path.join(results_directory,basename_genome_vcf) + ".gz > " + os.path.join(results_directory,basename_genome_vcf), "unzip genome vcf")

        if not os.path.lexists(os.path.join(results_directory,BASENAME_GENOME_VCF)):
            os.symlink(os.path.join(results_directory,basename_genome_vcf),
                       os.path.join(results_directory,BASENAME_GENOME_VCF))

        package_vcf(os.path.join(results_directory,BASENAME_VARIANTS_VCF))
        package_vcf(os.path.join(results_directory,BASENAME_GENOME_VCF))
        package_vcf(os.path.join(results_directory,basename_variants_vcf))
        package_vcf(os.path.join(results_directory,basename_genome_vcf))
        
        dataset_num += 1

def generate_hotspot_allele_cov(configuration, results_directory, processed_bam, untrimmed_bam):
    if configuration['options']['has_hotspots']:
        allelecount_command = 'samtools mpileup -BQ0 -d1000000'
        allelecount_command += ' -f "%s"' % configuration['options']['reference_genome_fasta']
        allelecount_command += ' -l ' + configuration['options']['hotspots_bed_merged']
        if configuration['options']['trim_reads']:
            allelecount_command += ' ' + processed_bam
        else:
            allelecount_command += ' ' + untrimmed_bam
        allelecount_command += ' | %s/scripts/allele_count_mpileup_stdin.py' % DIRNAME
        allelecount_command += ' > ' + os.path.join(results_directory,'allele_counts.txt')
        if PLUGIN_DEV_SKIP_VARIANT_CALLING:
            printtime('Skipping base pileup for hotspot alleles...')
        else:
            run_command(allelecount_command,'Base pileup for hotspot alleles')

        allelecount2_command = '%s/scripts/print_allele_counts.py' % DIRNAME
        allelecount2_command += ' ' + os.path.join(results_directory,'allele_counts.txt')
        allelecount2_command += ' ' + os.path.join(results_directory,BASENAME_HOTSPOTS_XLS)
        allelecount2_command += ' "%s"' % configuration['options']['hotspots_bed_unmerged_leftalign']
        allelecount2_command += ' "%s"' % configuration['options']['hotspots_bed_unmerged']
        run_command(allelecount2_command,'Generate hotspots allele coverage')

def generate_variant_tables(barcoded_run, configuration, results_directory, bam):
    tvc_args = configuration['json']['pluginconfig']['meta'].get('tvcargs','')
    table_command        = '%s/scripts/generate_variant_tables.py' % DIRNAME
    if tvc_args.find("--suppress-no-calls off") != -1: 
        table_command   += '  --suppress-no-calls off'
    else: 
        table_command   += '  --suppress-no-calls on'
    table_command       += '  --input-vcf %s'       % os.path.join(results_directory,BASENAME_VARIANTS_VCF)
    if configuration['options']['has_targets']:
        table_command   += '  --region-bed "%s"'    % configuration['options']['targets_bed_unmerged']
    if configuration['options']['has_hotspots']:
        table_command   += '  --hotspots'
    table_command       += '  --output-xls %s'      % os.path.join(results_directory,BASENAME_VARIANTS_XLS)
    table_command       += '  --alleles2-xls %s'    % os.path.join(results_directory,BASENAME_ALLELES_XLS)
    table_command       += '  --summary-json %s'    % os.path.join(results_directory,'variant_summary.json')
    table_command       += '  --scatter-png %s'     % os.path.join(results_directory,'scatter.png')
    if barcoded_run:
        table_command   += '  --barcode %s'  % bam['name']
        table_command   += '  --concatenated-xls "%s/%s.xls"' % (TSP_FILEPATH_PLUGIN_DIR,configuration['options']['run_name'])
    table_command       += '  --run-name "%s"'  % configuration['options']['run_name']
    table_command       += '  --library-type "%s"'  % configuration['options']['library_type']
    run_command(table_command,'Generate xls tables and statistics from final vcf')

def transfer_variants_to_sqlite(results_directory):
    subprocess.call('rm -f %s'     % os.path.join(results_directory,'alleles.db'), shell=True)
    (fin,unique_filename) = tempfile.mkstemp()
    sqllite_command        = 'python %s/scripts/csv2sqlite.py' % DIRNAME
    sqllite_command       += '  %s'     % os.path.join(results_directory,BASENAME_ALLELES_XLS)
    sqllite_command       += '  %s'     % unique_filename
    sqllite_command       += '  variants'
    run_command(sqllite_command,'Transfer variants to sqllite database')
    #copy sqllitedatabase to 
    copysql_command  = 'cp  %s  ' % unique_filename
    copysql_command += '  %s' %  os.path.join(results_directory,'alleles.db')
    run_command(copysql_command,'copy sqlite database to distination ')
    subprocess.call('rm -f %s'     % unique_filename, shell=True)
    os.chmod(os.path.join(results_directory,'alleles.db'), 0777)

def setup_webpage_support(results_directory):
    # Create symlinks to js/css folders and php scripts # static data
    subprocess.call('ln -sf "%s/slickgrid" "%s"' % (DIRNAME,results_directory),shell=True)
    subprocess.call('cp -rf %s/copytoreport/* "%s"' % (DIRNAME,results_directory),shell=True)
    subprocess.call('ln -sf %s/scripts/*.php3 "%s"' % (DIRNAME,results_directory),shell=True)
    
def load_render_context(barcoded_run, render_context, bam, results_directory, configuration):
    if barcoded_run:
        render_context['results_url'] += '/' + bam['name']

    summary_in = open(os.path.join(results_directory,'variant_summary.json'))
    render_context['summary'] = json.load(summary_in)
    summary_in.close()

    if configuration['options']['library_type'] == 'tagseq':
        tmol_summary = parse_to_dict( os.path.join(results_directory,'tmol.stats.txt'), ":" )
        #tmol_summary = render_context['summary']

        render_context['summary']['median_depth'] =  tmol_summary.get('Median read counts per target',"NA")
        render_context['summary']['median_num_fam3'] =  tmol_summary.get('median_num_fam3',"NA")
        render_context['summary']['fm3_pass80'] =  tmol_summary.get('fm3_pass80',"NA")
    else:
        render_context['summary']['median_depth'] = "NA"
        render_context['summary']['median_num_fam3'] = "NA"
        render_context['summary']['fm3_pass80'] = "NA"

    if configuration['options']['has_targets']:
        render_context['targets_bed_link'] = os.path.basename(configuration['options']['targets_bed_unmerged'])

    if configuration['options']['has_hotspots']:
        render_context['hotspots_bed_link'] = os.path.basename(configuration['options']['hotspots_bed_unmerged_local'])

    if configuration['options']['has_targets'] and configuration['options']['trim_reads']:
        render_context['effective_regions_bed_link'] = 'effective_regions.bed'

    if barcoded_run:
        render_context['barcode'] = bam['name']
        
def create_symlinks(barcoded_run, configuration, results_directory, render_context):
    if barcoded_run:
        if not os.path.lexists(os.path.join(results_directory,render_context['variants_vcf_gz_link'])):
            os.symlink(os.path.join(results_directory,BASENAME_VARIANTS_VCF+'.gz'),
                       os.path.join(results_directory,render_context['variants_vcf_gz_link']))
        if not os.path.lexists(os.path.join(results_directory,render_context['variants_tbi_link'])):
            os.symlink(os.path.join(results_directory,BASENAME_VARIANTS_VCF+'.gz.tbi'),
                       os.path.join(results_directory,render_context['variants_tbi_link']))
        if not os.path.lexists(os.path.join(results_directory,render_context['genome_vcf_gz_link'])):
            os.symlink(os.path.join(results_directory,BASENAME_GENOME_VCF+'.gz'),
                       os.path.join(results_directory,render_context['genome_vcf_gz_link']))
        if not os.path.lexists(os.path.join(results_directory,render_context['genome_tbi_link'])):
            os.symlink(os.path.join(results_directory,BASENAME_GENOME_VCF+'.gz.tbi'),
                       os.path.join(results_directory,render_context['genome_tbi_link']))
        if not os.path.lexists(os.path.join(results_directory,render_context['variants_xls_link'])):
            os.symlink(os.path.join(results_directory,BASENAME_VARIANTS_XLS),
                       os.path.join(results_directory,render_context['variants_xls_link']))
        if not os.path.lexists(os.path.join(results_directory,render_context['alleles_xls_link'])):
            os.symlink(os.path.join(results_directory,BASENAME_ALLELES_XLS),
                       os.path.join(results_directory,render_context['alleles_xls_link']))
        if configuration['options']['has_hotspots']:
            if not os.path.lexists(os.path.join(results_directory,render_context['hotspots_xls_link'])):
                os.symlink(os.path.join(results_directory,BASENAME_HOTSPOTS_XLS),
                           os.path.join(results_directory,render_context['hotspots_xls_link']))

def render_webpages(results_directory, render_context):
    out = open(results_directory + '/' + HTML_RESULTS,'w')
    out.write(render_to_string('report_details.html', render_context))
    out.close()

    out = open(results_directory + '/' + HTML_BLOCK,'w')
    out.write(render_to_string('block_details.html', render_context))
    out.close()

    out = open(results_directory + '/deprecated.htm','w')
    out.write(render_to_string('report_deprecated.html', render_context))
    out.close()

def generate_variant_allele_cov(configuration, results_directory, processed_bam, untrimmed_bam, bam, render_context):
    tvcutils_command = "tvcutils prepare_hotspots"
    tvcutils_command += ' --reference "%s"' % configuration['options']['reference_genome_fasta']
    tvcutils_command += ' --input-vcf "%s"' % os.path.join(results_directory,BASENAME_VARIANTS_VCF)
    tvcutils_command += ' --output-bed "%s"' % os.path.join(results_directory,'TSVC_variants.bed')
    run_command(tvcutils_command,'Write variants bed')
    allelecount_command = 'samtools mpileup -BQ0 -d1000000'
    allelecount_command += ' -f "%s"' % configuration['options']['reference_genome_fasta']
    allelecount_command += ' -l ' + os.path.join(results_directory,'TSVC_variants.bed')
    if configuration['options']['trim_reads']:
        allelecount_command += ' ' + processed_bam
    else:
        allelecount_command += ' ' + untrimmed_bam
    allelecount_command += ' | %s/scripts/allele_count_mpileup_stdin.py' % DIRNAME
    allelecount_command += ' > ' + os.path.join(results_directory,'TSVC_variants_allele_counts.txt')
    if PLUGIN_DEV_SKIP_VARIANT_CALLING:
        printtime('Skipping base pileup for variant alleles...')
    else:
        run_command(allelecount_command,'Base pileup for variant alleles')
    allelecount2_command = '%s/scripts/print_variant_allele_counts.py' % DIRNAME
    allelecount2_command += ' ' + bam['name']
    allelecount2_command += " '" + render_context['summary']['sample_name'] + "'"
    allelecount2_command += ' ' + os.path.join(results_directory,BASENAME_VARIANTS_VCF)
    allelecount2_command += ' ' + os.path.join(results_directory,'TSVC_variants_allele_counts.txt')
    allelecount2_command += ' ' + os.path.join(results_directory,BASENAME_VARIANT_COV_XLS)
    run_command(allelecount2_command,'Generate variant allele coverage')

def generate_xml_template_for_igv(configuration, barcode_modifier, basename_variants_vcf, processed_bam, untrimmed_bam, render_context, results_directory):
    # Create xml template required for adding IGV links
    fxml = open(os.path.join(results_directory,'igv_session.xml'), "w")
    fxml.write('<?xml version="1.0" encoding="UTF-8" standalone="no"?>\n')
    if configuration['options']['reference_genome_name'] == 'hg19':
        fxml.write('<Global genome="%s" version="3">\n' % configuration['options']['reference_genome_name'])
    else:    
        fxml.write('<Global genome="{plugin_url}/%s.fasta" version="3">\n' % (barcode_modifier + configuration['options']['reference_genome_name']))
    fxml.write('    <Resources>\n')
    fxml.write('        <Resource name="%s.gz" path="{plugin_url}/%s.gz"/>\n' % (basename_variants_vcf,basename_variants_vcf))
    if configuration['options']['trim_reads']:
        fxml.write('        <Resource name="%s" path="{plugin_url}/%s"/>\n' % (os.path.basename(processed_bam),os.path.basename(processed_bam)))
    else:
        fxml.write('        <Resource name="%s" path="{plugin_url}/%s"/>\n' % (os.path.basename(untrimmed_bam),os.path.basename(untrimmed_bam)))
    if configuration['options']['has_targets']:
        fxml.write('        <Resource name="%s" path="{plugin_url}/%s"/>\n' % (render_context['targets_bed_link'],barcode_modifier+render_context['targets_bed_link']))
    if configuration['options']['has_hotspots']:
        fxml.write('        <Resource name="%s" path="{plugin_url}/%s"/>\n' % (render_context['hotspots_bed_link'],barcode_modifier+render_context['hotspots_bed_link']))
    if configuration['options']['has_targets'] and configuration['options']['trim_reads']:
        fxml.write('        <Resource name="%s" path="{plugin_url}/%s"/>\n' % (render_context['effective_regions_bed_link'],render_context['effective_regions_bed_link']))
    fxml.write('    </Resources>\n')
    fxml.write('    <Panel name="DataPanel" height="150">\n')
    fxml.write('        <Track displayMode="EXPANDED" id="{plugin_url}/%s.gz" name="Variant Calls" visible="true"/>\n' % basename_variants_vcf)
    if configuration['options']['has_targets']:
        fxml.write('        <Track displayMode="COLLAPSED" id="{plugin_url}/%s" name="%s" visible="true"/>\n' % (barcode_modifier+render_context['targets_bed_link'],configuration['options']['targets_name']))
    if configuration['options']['has_hotspots']:
        fxml.write('        <Track displayMode="COLLAPSED" id="{plugin_url}/%s" name="%s" visible="true"/>\n' % (barcode_modifier+render_context['hotspots_bed_link'],configuration['options']['hotspots_name']))
    if configuration['options']['has_targets'] and configuration['options']['trim_reads']:
        fxml.write('        <Track displayMode="COLLAPSED" id="{plugin_url}/%s" name="%s" visible="true"/>\n' % (render_context['effective_regions_bed_link'],configuration['options']['targets_name'] + '_effective'))
    fxml.write('    </Panel>\n')
    fxml.write('    <Panel height="525">\n')
    if configuration['options']['trim_reads']:
        fxml.write('        <Track displayMode="COLLAPSED" id="{plugin_url}/%s_coverage" name="Coverage" visible="true"/>\n' % os.path.basename(processed_bam))
        fxml.write('        <Track displayMode="EXPANDED" id="{plugin_url}/%s" name="Alignments" visible="true"/>\n' % os.path.basename(processed_bam))
    else:
        fxml.write('        <Track displayMode="COLLAPSED" id="{plugin_url}/%s_coverage" name="Coverage" visible="true"/>\n' % os.path.basename(untrimmed_bam))
        fxml.write('        <Track displayMode="EXPANDED" id="{plugin_url}/%s" name="Alignments" visible="true"/>\n' % os.path.basename(untrimmed_bam))
    fxml.write('    </Panel>\n')
    fxml.write('    <Panel name="FeaturePanel" height="75">\n')
    fxml.write('        <Track displayMode="COLLAPSED" id="Reference sequence" name="Reference sequence" visible="true"/>\n')
    fxml.write('    </Panel>\n')
    fxml.write('    <PanelLayout dividerFractions="0.20,0.75"/>\n')
    fxml.write('</Global>\n')
    fxml.close()

def add_output_files(barcoded_run, configuration, bam, render_context):
    # List of generated files:
    if barcoded_run:
        add_output_file('variants_vcf_gz', bam['name']+'/'+render_context['variants_vcf_gz_link'], bam['name'], render_context['summary']['sample_name'])
        add_output_file('variants_vcf_gz_tbi', bam['name']+'/'+render_context['variants_tbi_link'], bam['name'], render_context['summary']['sample_name'])
        add_output_file('genome_vcf_gz', bam['name']+'/'+render_context['genome_vcf_gz_link'], bam['name'], render_context['summary']['sample_name'])
        add_output_file('genome_vcf_gz_tbi', bam['name']+'/'+render_context['genome_tbi_link'], bam['name'], render_context['summary']['sample_name'])
        add_output_file('alleles_xls', bam['name']+'/'+render_context['alleles_xls_link'], bam['name'], render_context['summary']['sample_name'])
        add_output_file('mapped_bam', bam['name']+'/'+render_context['mapped_bam_link'], bam['name'], render_context['summary']['sample_name'])
        add_output_file('mapped_bam_bai', bam['name']+'/'+render_context['mapped_bai_link'], bam['name'], render_context['summary']['sample_name'])
        if configuration['options']['trim_reads']:
            add_output_file('processed_bam', bam['name']+'/'+render_context['processed_bam_link'], bam['name'], render_context['summary']['sample_name'])
            add_output_file('processed_bam_bai', bam['name']+'/'+render_context['processed_bai_link'], bam['name'], render_context['summary']['sample_name'])
        add_output_file('filtered_variants_vcf', bam['name']+'/small_variants_filtered.vcf', bam['name'], render_context['summary']['sample_name'])
    else:
        add_output_file('variants_vcf_gz', render_context['variants_vcf_gz_link'], sample=render_context['summary']['sample_name'])
        add_output_file('variants_vcf_gz_tbi', render_context['variants_tbi_link'], sample=render_context['summary']['sample_name'])
        add_output_file('genome_vcf_gz', render_context['genome_vcf_gz_link'], sample=render_context['summary']['sample_name'])
        add_output_file('genome_vcf_gz_tbi', render_context['genome_tbi_link'], sample=render_context['summary']['sample_name'])
        add_output_file('alleles_xls', render_context['alleles_xls_link'], sample=render_context['summary']['sample_name'])
        add_output_file('mapped_bam', render_context['mapped_bam_link'], sample=render_context['summary']['sample_name'])
        add_output_file('mapped_bam_bai', render_context['mapped_bai_link'], sample=render_context['summary']['sample_name'])
        if configuration['options']['trim_reads']:
            add_output_file('processed_bam', render_context['processed_bam_link'], sample=render_context['summary']['sample_name'])
            add_output_file('processed_bam_bai', render_context['processed_bai_link'], sample=render_context['summary']['sample_name'])
        add_output_file('filtered_variants_vcf', 'small_variants_filtered.vcf', sample=render_context['summary']['sample_name'])

def process_results(barcoded_run, configuration, bam):
    if 'name' not in bam: 
        bam['name'] = 'default'
    if barcoded_run:
        barcode_modifier       = '../'
        basename_variants_vcf  = 'TSVC_variants_%s.vcf' % bam['name']
        basename_genome_vcf    = 'TSVC_variants_%s.genome.vcf' % bam['name']
        basename_variants_xls  = 'variants_%s.xls' % bam['name']
        basename_hotspots_xls  = 'allele_counts_%s.xls' % bam['name']
        basename_alleles_xls   = 'alleles_%s.xls' % bam['name']
        results_directory      = TSP_FILEPATH_PLUGIN_DIR + '/' + bam['name']
        basename_variant_cov_xls  = 'variant_allele_counts_%s.xls' % bam['name']
    else:
        barcode_modifier = ''
        basename_variants_vcf  = BASENAME_VARIANTS_VCF
        basename_genome_vcf    = BASENAME_GENOME_VCF
        basename_variants_xls  = BASENAME_VARIANTS_XLS
        basename_hotspots_xls  = BASENAME_HOTSPOTS_XLS
        basename_alleles_xls   = BASENAME_ALLELES_XLS
        results_directory      = TSP_FILEPATH_PLUGIN_DIR
        basename_variant_cov_xls  = BASENAME_VARIANT_COV_XLS

    if not os.path.exists(results_directory):
        os.makedirs(results_directory)

    basename_input_bam   = os.path.basename(bam['file'])
    untrimmed_bam = os.path.join(results_directory,basename_input_bam)                      # Local symlink to untrimmed BAM
    processed_bam = os.path.join(results_directory,basename_input_bam[:-4] + '_processed.bam')

    assert not (configuration['options']['trim_reads'] and not configuration['options']['has_targets']), "Read trimming enabled but targets BED not provided"

    generate_hotspot_allele_cov(configuration, results_directory, processed_bam, untrimmed_bam)
    generate_variant_tables(barcoded_run, configuration, results_directory, bam)
    transfer_variants_to_sqlite(results_directory)
    setup_webpage_support(results_directory)

    render_context = {
        'options'               : configuration['options'],
        'configuration_link'    : configuration['options']['parameters_file'],
        'mapped_bam_link'       : os.path.basename(untrimmed_bam),
        'mapped_bai_link'       : os.path.basename(untrimmed_bam)+'.bai',
        'variants_vcf_gz_link'  : basename_variants_vcf+'.gz',
        'variants_tbi_link'     : basename_variants_vcf+'.gz.tbi',
        'genome_vcf_gz_link'    : basename_genome_vcf+'.gz',
        'genome_tbi_link'       : basename_genome_vcf+'.gz.tbi',
        'variants_xls_link'     : basename_variants_xls,
        'alleles_xls_link'      : basename_alleles_xls,
        'hotspots_xls_link'     : basename_hotspots_xls,
        'variant_cov_xls_link'  : basename_variant_cov_xls,
        'processed_bam_link'    : os.path.basename(processed_bam),
        'processed_bai_link'    : os.path.basename(processed_bam)+'.bai',
        'results_url'           : TSP_URLPATH_PLUGIN_DIR,
        'startplugin_json'      : STARTPLUGIN_JSON
    }
    
    load_render_context(barcoded_run, render_context, bam, results_directory, configuration)
    create_symlinks(barcoded_run, configuration, results_directory, render_context)
    subprocess.call('touch %s/%s.done' % (results_directory,basename_variants_vcf),shell=True)
    render_webpages(results_directory, render_context)

    if barcoded_run:
        os.symlink(os.path.join(results_directory,BASENAME_VARIANT_COV_XLS),
                   os.path.join(results_directory,basename_variant_cov_xls))

    generate_variant_allele_cov(configuration, results_directory, processed_bam, untrimmed_bam, bam, render_context)
    generate_xml_template_for_igv(configuration, barcode_modifier, basename_variants_vcf, processed_bam, untrimmed_bam, render_context, results_directory)
    add_output_files(barcoded_run, configuration, bam, render_context)

    return render_context['summary']

def setup_results_json(barcoded_run):
    results_json = {
        'files'             : []
    }
    if barcoded_run:
        results_json['barcoded'] = 'true'
        results_json['barcodes'] = {}
    else:
        results_json['barcoded'] = 'false'
    return results_json

def load_barcode_sample_info(barcoded_run, configuration):
    barcode_sample_info = {}
    if barcoded_run and configuration['options']['start_mode'] == 'Auto start':
        samples = STARTPLUGIN_JSON.get('plan',{}).get('barcodedSamples',"")
        if samples and not isinstance(samples,dict):
            samples = json.loads(samples)
    
        for key, value in samples.iteritems():
            barcode_sample_info.update(value.get('barcodeSampleInfo',{}))
    return barcode_sample_info

def set_from_plan(configuration, bam, barcode_sample_info):
    if configuration['options']['start_mode'] == 'Auto start' and bam['name'] in barcode_sample_info:
        configuration['options']['sample_name']           = barcode_sample_info[bam['name']].get('sample','')
        configuration['options']['reference_genome_name'] = barcode_sample_info[bam['name']].get('reference','')
        configuration['options']['targets_bed_unmerged']  = barcode_sample_info[bam['name']].get('targetRegionBedFile','')
        configuration['options']['hotspots_bed_unmerged'] = barcode_sample_info[bam['name']].get('hotSpotRegionBedFile','')
        configuration['options']['nuc_type']              = barcode_sample_info[bam['name']].get('nucleotideType','DNA').upper()
    else:
        if bam['name'] not in barcode_sample_info: 
            printtime('Detected barcode ' + bam['name'] + ' : Missing barcode sample info ')
        else:
            configuration['options']['sample_name'] = barcode_sample_info[bam['name']].get('sample','')

def write_parameters_file(configuration_name, configuration):
    if configuration_name == '':
        configuration['options']['parameters_file'] = os.path.join(TSP_FILEPATH_PLUGIN_DIR, "local_parameters.json")
    else:
        configuration['options']['parameters_file'] = os.path.join(TSP_FILEPATH_PLUGIN_DIR, configuration_name + "_parameters.json")
    fin = open(configuration['options']['parameters_file'],'w')
    json.dump(configuration['options']['parameters'],fin,indent=4)
    fin.close()
    add_output_file('parameters_json', configuration_name + '_parameters.json')

def load_results_json(barcoded_run, configuration, bam, results_json):
    try:
        fin = open(bam['vc_pipeline_directory'] + "/tvc_metrics.json", 'r')
        metrics = json.load(fin,parse_float=str)
        fin.close()
    except:
        metrics = {}
        
    if barcoded_run:
        results_json['barcodes'][bam['name']] = {}
        results_json['barcodes'][bam['name']]['Aligned Reads'] = configuration['options']['run_name']
        results_json['barcodes'][bam['name']]['Library Type'] = configuration['options']['library_type']
        results_json['barcodes'][bam['name']]['Configuration'] =  configuration['options']['parameters']['meta']['configuration']
        if configuration['options']['has_targets']:
            results_json['barcodes'][bam['name']]['targets_bed']  = configuration['options']['targets_bed_unmerged']
        if configuration['options']['has_hotspots']:
            results_json['barcodes'][bam['name']]['hotspots_bed'] = configuration['options']['hotspots_bed_unmerged']
        results_json['barcodes'][bam['name']]['Trim Reads'] = configuration['options']['trim_reads']
        results_json['barcodes'][bam['name']]['variants'] = bam['summary'].get('variants_total',{})
        results_json['barcodes'][bam['name']]['hotspots'] = bam['summary'].get('hotspots_total',{})
        results_json['barcodes'][bam['name']]['deamination_metric'] = metrics.get('metrics',{}).get('deamination_metric',0)
    else:
        results_json['Aligned Reads'] = configuration['options']['run_name']
        results_json['Library Type'] = configuration['options']['library_type']
        results_json['Configuration'] =  configuration['options']['parameters']['meta']['configuration']
        results_json['Target Regions'] = (configuration['options']['targets_name'] if configuration['options']['has_targets'] else 'Not using')
        results_json['Target Loci'] =  (configuration['options']['hotspots_name'] if configuration['options']['has_hotspots'] else 'Not using')
        results_json['Trim Reads'] = configuration['options']['trim_reads']
        results_json['variants'] = bam['summary'].get('variants_total',{})
        results_json['hotspots'] = bam['summary'].get('hotspots_total',{})
        results_json['deamination_metric'] = metrics.get('metrics',{}).get('deamination_metric',0)

def generate_download_files(run_name, bams_processed):
    printtime(' ')
    printtime('Task    : ' + 'Store per-barcode vcf files in a single zip file')
    zipfilename = '%s/%s.vcf.zip' % (TSP_FILEPATH_PLUGIN_DIR,run_name)
    for myfile in [('%s/%s/TSVC_variants_%s.vcf.gz' % (TSP_FILEPATH_PLUGIN_DIR,bam['name'],bam['name'])) for bam in bams_processed if bam['status'] == 'completed']:
        compress.make_zip(zipfilename, myfile, arcname=os.path.basename(myfile), use_sys_zip = False)
    for myfile in [('%s/%s/TSVC_variants_%s.vcf.gz.tbi' % (TSP_FILEPATH_PLUGIN_DIR,bam['name'],bam['name'])) for bam in bams_processed if bam['status'] == 'completed']:
        compress.make_zip(zipfilename, myfile, arcname=os.path.basename(myfile), use_sys_zip = False)
    printtime(' ')
    printtime(' ')
    printtime('Task    : ' + 'Store per-barcode xls files in a single zip file')
    zipfilename = '%s/%s.xls.zip' % (TSP_FILEPATH_PLUGIN_DIR,run_name)
    for myfile in [('%s/%s/alleles_%s.xls' % (TSP_FILEPATH_PLUGIN_DIR,bam['name'],bam['name'])) for bam in bams_processed if bam['status'] == 'completed']:
        compress.make_zip(zipfilename, myfile, arcname=os.path.basename(myfile), use_sys_zip = False)
    printtime(' ')
    printtime('Task    : ' + 'Store per-barcode cov files in a single zip file')
    combinedfilename = '%s/%s.cov.xls' % (TSP_FILEPATH_PLUGIN_DIR,run_name)
    for myfile in [('%s/%s/variant_allele_counts_%s.xls' % (TSP_FILEPATH_PLUGIN_DIR,bam['name'],bam['name'])) for bam in bams_processed if bam['status'] == 'completed']:
        combine_files(combinedfilename, myfile)
    printtime(' ')

def write_results_json(results_json):
    results_json['files'] = OUTPUT_FILES
    out = open(TSP_FILEPATH_PLUGIN_DIR + '/results.json','w')
    json.dump(results_json,out,indent=4)
    out.close()
    
def setup_run():
    settings.configure(DEBUG=True, TEMPLATE_DEBUG=True, TEMPLATE_DIRS=((DIRNAME+'/templates'),))

    subprocess.call('rm -f %s/results.json' % TSP_FILEPATH_PLUGIN_DIR,shell=True)

    # Make links to js/css used for barcodes table and empty results page
    subprocess.call('ln -sf "%s/js" "%s"' % (DIRNAME,TSP_FILEPATH_PLUGIN_DIR),shell=True)
    subprocess.call('ln -sf "%s/css" "%s"' % (DIRNAME,TSP_FILEPATH_PLUGIN_DIR),shell=True)
    subprocess.call('ln -sf %s/scripts/*.php3 "%s"' % (DIRNAME,TSP_FILEPATH_PLUGIN_DIR),shell=True)

def process_configuration(barcoded_run, multisample, configuration_name, configuration, results_json, bams_processed):
    if 'error' in configuration['options']:
        printtime(configuration['options']['error'])
        for bam in configuration['bams']:
            bam['status'] = 'error'
            bams_processed.append(bam)
        return
        
    barcode_sample_info = load_barcode_sample_info(barcoded_run, configuration)
    
    write_parameters_file(configuration_name, configuration)

    if configuration['options']['library_type'] == 'tagseq':
        if not configuration['options']['has_targets']:
            printtime('ERROR: Analysis aborted: TagSequencing runs must have target regions specified.')
            for bam in configuration['bams']:
               bam['status'] = 'error'
               bams_processed.append(bam)
            return
        if not configuration['options']['has_hotspots']:
            printtime('ERROR: Analysis aborted: TagSequencing runs must have hotspot file specified.')
            for bam in configuration['bams']:
               bam['status'] = 'error'
               bams_processed.append(bam)
            return

    for bam in configuration['bams']:
        printtime("name: %s" % bam['name'])
        bam['status'] = 'in_progress'
        if configuration_name == "":
            print_options(configuration['options'], "local_parameters.json")
        else:
            print_options(configuration['options'], configuration_name + "_parameters.json")

    for bam in configuration['bams']:
        set_from_plan(configuration, bam, barcode_sample_info)
        process_reference(configuration, bam)
    
    if multisample:
        try:
            variant_caller_pipeline(barcoded_run, multisample, configuration_name, configuration, configuration['bams'])
            split_results(barcoded_run, configuration_name, configuration, configuration['bams'])
        except:
            traceback.print_exc()
            for bam in configuration['bams']:
                bam['status'] = 'error'
    else:
        for bam in configuration['bams']:
            bams = []
            bams.append(bam)
            try:
                variant_caller_pipeline(barcoded_run, multisample, bam['name'], configuration, bams)
            except:
                traceback.print_exc()
                bam['status'] = 'error'

    for bam in configuration['bams']:
        try:
            bam['summary'] = process_results(barcoded_run, configuration, bam)
            load_results_json(barcoded_run, configuration, bam, results_json)
            bam['status'] = 'completed'
            bams_processed.append(bam)
        except:
            traceback.print_exc()
            bam['status'] = 'error'
            bams_processed.append(bam)
        
def plugin_main():
    global PLUGIN_DEV_SKIP_VARIANT_CALLING
    global DIRNAME
    global TSP_URLPATH_PLUGIN_DIR
    global ANALYSIS_DIR
    global TSP_FILEPATH_PLUGIN_DIR
    global STARTPLUGIN_JSON
    global OUTPUT_FILES

    parser = OptionParser()
    parser.add_option('-d', '--install-dir', help='Directory containing plugin files', dest='install_dir')
    parser.add_option('-o', '--output-dir', help='Directory for results files', dest='output_dir')
    parser.add_option('-u', '--output-url', help='URL matching the output directory', dest='output_url')
    parser.add_option('-r', '--report-dir', help='Directory containing analysis report files', dest='report_dir')
    parser.add_option('-s', '--skip-tvc', help='(debug) Skip variant calling and reuse existing results', dest='skip_tvc', action="store_true", default=False)
    (options, args) = parser.parse_args()

    DIRNAME                     = options.install_dir    #os.environ['DIRNAME']         # home directory for the plugin files
    TSP_FILEPATH_PLUGIN_DIR     = options.output_dir     #os.environ['TSP_FILEPATH_PLUGIN_DIR'] # target plugin results directory
    ANALYSIS_DIR                = options.report_dir     #os.environ['ANALYSIS_DIR'] # main report directory
    TSP_URLPATH_PLUGIN_DIR      = options.output_url
    PLUGIN_DEV_SKIP_VARIANT_CALLING = options.skip_tvc
    OUTPUT_FILES = []
    
    setup_run()

    printtime('')
    printtime('Variant Caller Plugin started')
    printtime('')

    printtime('Loading ' + os.path.join(TSP_FILEPATH_PLUGIN_DIR,'startplugin.json'))
    try:
        json_file = open(os.path.join(TSP_FILEPATH_PLUGIN_DIR,'startplugin.json'), 'r')
        STARTPLUGIN_JSON = json.load(json_file,parse_float=str)
        json_file.close()
    except:
        printtime('ERROR: Failed to load and parse startplugin.json')
        return 1

    run_name = STARTPLUGIN_JSON['expmeta'].get('run_name','Current run')
    (configured_run, barcoded_run, configurations, multisample) = get_configurations()

    results_json = setup_results_json(barcoded_run)

    bams_processed = []
    for configuration_name in configurations:
        configuration = configurations[configuration_name]
        configuration['options'] = get_options(configuration['json'])
        configuration['options']['configurations'] = configured_run
        process_configuration(barcoded_run, multisample, configuration_name, configuration, results_json, bams_processed)

    if barcoded_run:
        generate_download_files(run_name, bams_processed)
        generate_barcode_links_page(os.path.join(TSP_FILEPATH_PLUGIN_DIR,HTML_RESULTS), bams_processed, configuration['options'])
        generate_barcode_links_block(os.path.join(TSP_FILEPATH_PLUGIN_DIR,HTML_BLOCK), bams_processed, configuration['options'])
    else:
        for bam in bams_processed:
            if bam['status'] == 'error':
                generate_incomplete_report_page(os.path.join(TSP_FILEPATH_PLUGIN_DIR,HTML_RESULTS), 'An error occurred - check Log File for details', configuration['options'])
                break

    for bam in bams_processed:
        if bam['status'] == 'error':
            return 1
            
    write_results_json(results_json)

    printtime('')
    printtime('Variant Caller Plugin complete')
    printtime('')
    
    return 0

if __name__ == "__main__":

    exit(plugin_main())


